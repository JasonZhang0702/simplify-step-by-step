{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### The following code is used for choosing optimal cefr predictor for CEFR-SP corpus",
   "id": "91fd70cfeaf27f70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Loading packages",
   "id": "2e596937ed0c46d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from easse.fkgl import corpus_fkgl\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import BertForMaskedLM, AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import re\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')"
   ],
   "id": "bdaae9b49e44b720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_path', default = 'C:/PLMs/xlm-roberta-base')\n",
    "parser.add_argument('--device', default = 'cuda')\n",
    "args = parser.parse_args()\n",
    "model = AutoModelForMaskedLM.from_pretrained(args.model_path).to(args.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
    "model.eval()"
   ],
   "id": "b7282b0c84eedf0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_ari(text):\n",
    "    \"\"\"\n",
    "    计算文本的 Automated Readability Index (ARI) 可读性指标\n",
    "    :param text: 输入文本（字符串）\n",
    "    :return: ARI 值（浮点数）\n",
    "    \"\"\"\n",
    "    # 分割句子（按句号、问号、感叹号分割）\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]  # 去除空句子\n",
    "\n",
    "    if not sentences:\n",
    "        raise ValueError(\"文本中没有有效的句子\")\n",
    "\n",
    "    total_characters = 0\n",
    "    total_words = 0\n",
    "\n",
    "    # 遍历每个句子，统计字符数和单词数\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()  # 按空格分割单词\n",
    "        if not words:\n",
    "            continue  # 跳过空句子\n",
    "\n",
    "        total_words += len(words)\n",
    "        for word in words:\n",
    "            total_characters += len(word)  # 统计字符数（包括符号、数字等）\n",
    "\n",
    "    if total_words == 0:\n",
    "        raise ValueError(\"文本中没有有效的单词\")\n",
    "\n",
    "    # 计算 AWL 和 ASL\n",
    "    awl = total_characters / total_words\n",
    "    asl = total_words / len(sentences)\n",
    "\n",
    "    # 计算 ARI\n",
    "    ari = 4.71 * awl + 0.5 * asl - 21.43\n",
    "    return ari"
   ],
   "id": "75ef9c1b2eed8845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_wnll(yt, yp):\n",
    "    wnll = -(yt.dot(torch.log(yp)) + (1-yt).dot(torch.log(1-yp)))\n",
    "    if torch.isnan(wnll): return 1e-10\n",
    "    return float(wnll)\n",
    "\n",
    "\n",
    "def compute_rsrs(wnll):\n",
    "    wnll.sort()\n",
    "    rsrs = 0\n",
    "    for i, w in enumerate(wnll):\n",
    "        squ_root = math.sqrt(i + 1)\n",
    "        rsrs += squ_root*w\n",
    "    return rsrs/len(wnll)\n",
    "\n",
    "\n",
    "def compute_readability(sentence):\n",
    "    WNLL = []\n",
    "    sentence_tokenized = word_tokenize(sentence)\n",
    "    sentence_batch = []\n",
    "    for id, token in enumerate(sentence_tokenized):\n",
    "        sentence_mask = sentence_tokenized[:id] + [tokenizer.special_tokens_map[\"mask_token\"]] + sentence_tokenized[id + 1:]\n",
    "        sentence_mask = \" \".join(sentence_mask)\n",
    "        sentence_batch.append(sentence_mask)\n",
    "\n",
    "    inputs = tokenizer(sentence_batch, return_tensors = 'pt', padding = True).to(args.device)\n",
    "    row_indices, col_indices = torch.where(inputs.input_ids == tokenizer.vocab[tokenizer.special_tokens_map[\"mask_token\"]])  # getting MASK token index\n",
    "    output = model(**inputs).logits\n",
    "    for oid, mask_id in enumerate(col_indices):\n",
    "        yp = F.softmax(output[oid, mask_id, :], dim = 0)  # getting MASK token probability\n",
    "        yt = torch.zeros(len(tokenizer)).to(args.device)\n",
    "        orig_token_id = tokenizer.encode(sentence_tokenized[oid], add_special_tokens=False)\n",
    "        yt[orig_token_id] = 1\n",
    "        wnll = compute_wnll(yt, yp)\n",
    "        WNLL.append(wnll)\n",
    "    assert len(WNLL) == len(sentence_tokenized)\n",
    "    return compute_rsrs(WNLL)"
   ],
   "id": "ecf6c7c1d5e66c99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing CEFR-SP corpus",
   "id": "b1d5970563f3c3a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "USING_WHOLE = False\n",
    "if USING_WHOLE: df = pd.read_csv(\"CEFR-SP_test_whole.csv\")\n",
    "else: df = pd.read_csv(\"CEFR-SP_test_part.csv\")\n",
    "\n",
    "Sentence = df[\"Sentence\"].tolist()\n",
    "Label1 = np.array(df[\"Label1\"].tolist())\n",
    "Label2 = np.array(df[\"Label2\"].tolist())"
   ],
   "id": "925b4e883b38a85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### calculating FKGL and analyzing correlation",
   "id": "2c04e38bc82df170"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prediction = [corpus_fkgl([sent]) for sent in Sentence]\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "corr1, _ = pearsonr(Label1, prediction)\n",
    "corr2, _ = pearsonr(Label2, prediction)\n",
    "print(\"Corr1: \", round(corr1, 3))   # taking the higher correlation-value\n",
    "print(\"Corr2: \", round(corr2, 3))"
   ],
   "id": "1e5ecfdc2c00be62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### calculating ARI and analyzing correlation",
   "id": "404eb2c27871d0ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prediction = [compute_ari([sent]) for sent in Sentence]\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "corr1, _ = pearsonr(Label1, prediction)\n",
    "corr2, _ = pearsonr(Label2, prediction)\n",
    "print(\"Corr1: \", round(corr1, 3))   # taking the higher correlation-value\n",
    "print(\"Corr2: \", round(corr2, 3))"
   ],
   "id": "c0c168de597bdecd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### calculating SL and analyzing correlation",
   "id": "b0987675af2c50ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prediction = [len([sent]) for sent in Sentence]\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "corr1, _ = pearsonr(Label1, prediction)\n",
    "corr2, _ = pearsonr(Label2, prediction)\n",
    "print(\"Corr1: \", round(corr1, 3))   # taking the higher correlation-value\n",
    "print(\"Corr2: \", round(corr2, 3))"
   ],
   "id": "9602f32d7da3a87f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### calculating RSRS and analyzing correlation",
   "id": "ed790aa232d80343"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prediction = [compute_readability(sent) for sent in tqdm(Sentence)]\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "corr1, _ = pearsonr(Label1, prediction)\n",
    "corr2, _ = pearsonr(Label2, prediction)\n",
    "print(\"Corr1: \", round(corr1, 3))   # taking the higher correlation-value\n",
    "print(\"Corr2: \", round(corr2, 3))"
   ],
   "id": "13280f1a0759e21b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
